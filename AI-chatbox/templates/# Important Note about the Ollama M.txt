# Important Note about the Ollama Model File

The Ollama model (`gemma3`) used in this project is **approximately 4GB in size**. Due to size constraints, the model files themselves are **not included in this repository**.

## How to Set Up the Model Locally

1. **Install Ollama**: Download and install the Ollama software from [https://ollama.com/](https://ollama.com/).

2. **Download the `gemma3` Model**:
   - Use Ollama's official commands or interface to download the `gemma3` model.
   - For example, run:
     ```bash
     ollama pull gemma3:latest
     ```
   - This will download the model to your local machine via Ollama.

3. **Run the Model Locally**:
   - Start the model API server by running:
     ```bash
     ollama run gemma3
     ```
   - Ensure the API endpoint `http://localhost:11434/api/generate` is available.

4. **Run the Flask App**:
   - Start the Flask backend with:
     ```bash
     python app.py
     ```
   - Open your browser and access the frontend at `http://localhost:5000/`.
